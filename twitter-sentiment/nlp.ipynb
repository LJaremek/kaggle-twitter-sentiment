{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import LancasterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "# [ models for classification ]\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepearing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path: str,\n",
    "              delimiter: str = \",\",\n",
    "              headers: bool = True) -> pd.DataFrame:\n",
    "    if headers:\n",
    "        return pd.read_csv(file_path, sep=delimiter)\n",
    "    return pd.read_csv(file_path, sep=delimiter, header=None)\n",
    "\n",
    "\n",
    "def clean_sentence(text: str) -> str:\n",
    "    text = re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\S+\", \"\", text)\n",
    "    letters = list(\" qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\")\n",
    "    for symbol in text:\n",
    "        if symbol not in letters:\n",
    "            text = text.replace(symbol, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_sentences(sentences_list: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     * sentences_list: list[str] - list of sentences\n",
    "\n",
    "    Output:\n",
    "     * list[list[str]] - list of cleansed sentences (without special symbols)\n",
    "\n",
    "    Example:\n",
    "        Input: [\"Hello my World!\",\n",
    "                \"How are you?\"]\n",
    "\n",
    "        Output: [[\"Hello\", \"my\", \"World\"],\n",
    "                 [\"How\", \"are\", \"you\"]]\n",
    "    \"\"\"\n",
    "    cleansed_sentences = []\n",
    "    for tweet in [clean_sentence(tweet).split(\" \")\n",
    "                  for tweet in sentences_list]:\n",
    "\n",
    "        cleansed_sentence = [word.strip() for word in tweet\n",
    "                             if\n",
    "                             word != \"\" and\n",
    "                             \"http\" not in word and\n",
    "                             not word.isdigit()]\n",
    "\n",
    "        cleansed_sentences.append(cleansed_sentence)\n",
    "\n",
    "    return cleansed_sentences\n",
    "\n",
    "\n",
    "def flat_lists(sentences_list: list[list[str]]) -> list[str]:\n",
    "    words = []\n",
    "    for sentence in sentences_list:\n",
    "        words += sentence\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \n",
      "0      Eastern Time (US & Canada)  \n",
      "1      Pacific Time (US & Canada)  \n",
      "2      Central Time (US & Canada)  \n",
      "3      Pacific Time (US & Canada)  \n",
      "4      Pacific Time (US & Canada)  \n",
      "...                           ...  \n",
      "14635                         NaN  \n",
      "14636                         NaN  \n",
      "14637                         NaN  \n",
      "14638  Eastern Time (US & Canada)  \n",
      "14639                         NaN  \n",
      "\n",
      "[14640 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# [ loading data ]\n",
    "all_data_frame = read_data(\"data/Tweets.csv\")\n",
    "print(all_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0                    @VirginAmerica What @dhepburn said.\n",
      "1      @VirginAmerica plus you've added commercials t...\n",
      "2      @VirginAmerica I didn't today... Must mean I n...\n",
      "3      @VirginAmerica it's really aggressive to blast...\n",
      "4      @VirginAmerica and it's a really big bad thing...\n",
      "...                                                  ...\n",
      "14635  @AmericanAir thank you we got on a different f...\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...\n",
      "14637  @AmericanAir Please bring American Airlines to...\n",
      "14638  @AmericanAir you have my money, you change my ...\n",
      "14639  @AmericanAir we have 8 ppl so we need 2 know h...\n",
      "\n",
      "[14640 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "airline_sentiment = all_data_frame[\"airline_sentiment\"]\n",
    "data_frame = pd.DataFrame({\"text\": all_data_frame[\"text\"]})\n",
    "print(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0                                           [What, said]\n",
      "1      [plus, youve, added, commercials, to, the, exp...\n",
      "2      [I, didnt, today, Must, mean, I, need, to, tak...\n",
      "3      [its, really, aggressive, to, blast, obnoxious...\n",
      "4      [and, its, a, really, big, bad, thing, about, it]\n",
      "...                                                  ...\n",
      "14635  [thank, you, we, got, on, a, different, flight...\n",
      "14636  [leaving, over, minutes, Late, Flight, No, war...\n",
      "14637  [Please, bring, American, Airlines, to, BlackB...\n",
      "14638  [you, have, my, money, you, change, my, flight...\n",
      "14639  [we, have, ppl, so, we, need, know, how, many,...\n",
      "\n",
      "[14640 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data_frame = pd.DataFrame({\"text\": clean_sentences(data_frame[\"text\"])})\n",
    "cleansed_words = flat_lists(cleaned_data_frame[\"text\"])\n",
    "print(cleaned_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_sentence(sentence: list[str],\n",
    "                           lancaster: LancasterStemmer) -> list[str]:\n",
    "\n",
    "    return [lancaster.stem(word) for word in sentence]\n",
    "\n",
    "\n",
    "def lemmatization_sentences(sentences: list[list[str]],\n",
    "                            lancaster: LancasterStemmer) -> list[list[str]]:\n",
    "\n",
    "    return [lemmatization_sentence(sentence, lancaster) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0                                           [what, said]\n",
      "1       [plu, youv, ad, commerc, to, the, expery, tacky]\n",
      "2      [i, didnt, today, must, mean, i, nee, to, tak,...\n",
      "3      [it, real, aggress, to, blast, obnoxy, enterta...\n",
      "4         [and, it, a, real, big, bad, thing, about, it]\n",
      "...                                                  ...\n",
      "14635  [thank, you, we, got, on, a, diff, flight, to,...\n",
      "14636  [leav, ov, minut, lat, flight, no, warn, or, c...\n",
      "14637         [pleas, bring, am, airlin, to, blackberry]\n",
      "14638  [you, hav, my, money, you, chang, my, flight, ...\n",
      "14639  [we, hav, ppl, so, we, nee, know, how, many, s...\n",
      "\n",
      "[14640 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed_data_frame = pd.DataFrame({\"text\": lemmatization_sentences(cleaned_data_frame[\"text\"], lancaster)})\n",
    "print(stemmed_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counf of stemmed words: 233344\n",
      "['latinam', 'gott', 'desert', 'callback', 'annnnnd', 'elevategold', 'washington', 'circ', 'pandora', 'trit', 'helpa', 'overkil', 'leathers', 'thrilled', 'mirand', 'ic', 'fresh', 'planeso', 'americanair', 'nick', 'happyfriday', 'sel', 'clehelp', 'shaquil', 'institut', 'bledso', 'hopetogetanswersoon', 'sheil', 'inperson', 'troy', 'wheez', 'watson', 'country', 'directb', 'problemss', 'denewr', 'mccarran', 'ward', 'superst', 'upin', 'stick', 'stellarserv', 'mewh', 'priortry', 'brokenwheel', 'myself', 'holy', 'bed', 'trueblu', 'miami', 'ref', 'princesshalf', 'aft', 'chronological', 'fli', 'buy', 'tomorro', 'delyd', 'taxy', 'hel', 'triv', 'ph', 'outpost', 'interview', 'nondelay', 'febru', 'scumb', 'cash', 'facebook', 'shift', 'alcohol', 'k', 'downnnn', 'lifeisgood', 'vil', 'yearround', 'linkemail', 'theworstairlineev', 'jack', 'cough', 'giv', 'nonsens', 'curs', 'piel', 'bei', 'sweresomuchfun', 'laxjfk', 'eqm', 'realtim', 'flgjt', 'justdippin', 'teem', 'disspoint', 'minutessaid', 'crawl', 'cstmr', 'attir', 'audio', 'und', 'mediocr']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = flat_lists(stemmed_data_frame[\"text\"])\n",
    "bag_of_words = list(set(stemmed_words))\n",
    "print(\"Counf of stemmed words:\", len(stemmed_words))\n",
    "print(bag_of_words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_coding(sentence: list[str],\n",
    "                    bag_of_words: list[str]) -> list[bool]:\n",
    "    # 1 - word occurs in the bag of words\n",
    "    # 0 - word does not appear in the sentence\n",
    "    return [1 if word in sentence else 0 for word in bag_of_words]\n",
    "\n",
    "\n",
    "def sentences_coding(sentences: list[list[str]],\n",
    "                     bag_of_words: list[str]) -> list[bool]:\n",
    "    return [sentence_coding(sentence, bag_of_words) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1     2     3     4     5     6     7     8     9     ...  8997  \\\n",
      "0         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4         0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "14635     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "14636     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "14637     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "14638     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "14639     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "       8998  8999  9000  9001  9002  9003  9004  9005  9006  \n",
      "0         0     0     0     0     0     0     0     0     0  \n",
      "1         0     0     0     0     0     0     0     0     0  \n",
      "2         0     0     0     0     0     0     0     0     0  \n",
      "3         0     0     0     0     0     0     0     0     0  \n",
      "4         0     0     0     0     0     0     0     0     0  \n",
      "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "14635     0     0     0     0     0     0     0     0     0  \n",
      "14636     0     0     0     0     0     0     0     0     0  \n",
      "14637     0     0     0     0     0     0     0     0     0  \n",
      "14638     0     0     0     0     0     0     0     0     0  \n",
      "14639     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[14640 rows x 9007 columns]\n"
     ]
    }
   ],
   "source": [
    "# [ change words for numbers ]\n",
    "coded_data_frame = pd.DataFrame(\n",
    "   sentences_coding(stemmed_data_frame[\"text\"], bag_of_words)\n",
    "    )\n",
    "\n",
    "print(coded_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6     \\\n",
      "0     -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "1     -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "2     -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "3     -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "4     -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14635 -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "14636 -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "14637 -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "14638 -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "14639 -0.008265 -0.032026 -0.011689 -0.037901 -0.008265 -0.008265 -0.029812   \n",
      "\n",
      "           7         8         9     ...      8997      8998      8999  \\\n",
      "0     -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "1     -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "2     -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "3     -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "4     -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "14635 -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "14636 -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "14637 -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "14638 -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "14639 -0.021872 -0.008265 -0.008265  ... -0.008265 -0.042985 -0.008265   \n",
      "\n",
      "           9000      9001      9002      9003      9004      9005      9006  \n",
      "0     -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "1     -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "2     -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "3     -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "4     -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "14635 -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "14636 -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "14637 -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "14638 -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "14639 -0.008265 -0.008265 -0.014316 -0.008265 -0.038794 -0.008265 -0.014316  \n",
      "\n",
      "[14640 rows x 9007 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "normal_data_frame = pd.DataFrame(\n",
    "    scaler.fit_transform(coded_data_frame)\n",
    "    )\n",
    "\n",
    "print(normal_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normal_data_frame, airline_sentiment, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 200, 150, 100, 50),\n",
    "    batch_size=200,\n",
    "    max_iter=400,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=200, hidden_layer_sizes=(150, 200, 150, 100, 50),\n",
       "              learning_rate_init=0.01, max_iter=400, random_state=1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_y_pred = mlp_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.90      0.82      1857\n",
      "     neutral       0.55      0.44      0.49       589\n",
      "    positive       0.74      0.39      0.51       482\n",
      "\n",
      "    accuracy                           0.72      2928\n",
      "   macro avg       0.68      0.58      0.61      2928\n",
      "weighted avg       0.71      0.72      0.71      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, mlp_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred = rf_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.96      0.85      1857\n",
      "     neutral       0.68      0.38      0.49       589\n",
      "    positive       0.88      0.49      0.63       482\n",
      "\n",
      "    accuracy                           0.77      2928\n",
      "   macro avg       0.78      0.61      0.66      2928\n",
      "weighted avg       0.77      0.77      0.74      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gauss_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model_y_pred = gauss_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.34      0.49      1857\n",
      "     neutral       0.26      0.28      0.27       589\n",
      "    positive       0.23      0.74      0.36       482\n",
      "\n",
      "    accuracy                           0.40      2928\n",
      "   macro avg       0.44      0.46      0.37      2928\n",
      "weighted avg       0.62      0.40      0.42      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, gauss_model_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_model = BernoulliNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_model_y_pred = bernoulli_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.92      0.88      1857\n",
      "     neutral       0.61      0.55      0.58       589\n",
      "    positive       0.80      0.58      0.67       482\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.75      0.68      0.71      2928\n",
      "weighted avg       0.78      0.79      0.78      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, bernoulli_model_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6724726775956285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jarem\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = logreg.score(X_test, y_test)\n",
    "\n",
    "print(\"Logistic Regression accuracy:\", res)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
